---
date: 2026-01-12
topic: "Adding Context and Impact to Portfolio Vignettes"
tags: [research, portfolio, feedback-synthesis, hiring-manager-perspective, case-study-design]
status: in-progress
last_updated: 2026-01-12
last_updated_by: claude
---

# Research: Adding Context and Impact to Portfolio Vignettes

Three design professionals (2 design managers, 1 lead designer) reviewed the portfolio and rated it 3-4/5. The craft and interactive approach are praised, but all three independently identified the same gap: **the work shows what was built, but not the thinking, constraints, or outcomes that would signal lead-level design.**

## Core Finding

The portfolio currently reads as "highly skilled senior" rather than "lead." This isn't about adding more content—it's about reframing existing content to show influence, decisions, and impact.

| What Exists | What's Missing |
|-------------|----------------|
| Problem statement (1 sentence) | Business context, constraints, urgency |
| Solution statement (1 sentence) | Trade-offs considered, decisions made |
| Design notes (UI annotations) | Process insights, research findings |
| Adoption stats (some vignettes) | Outcomes, behavioral shifts, strategic impact |
| Strong craft | Evidence of leadership and influence |

---

## Feedback Synthesis

### Reviewer 1 (Rating: 4/5)
> "Getting a sense of the setting, the challenges, business context and maybe describe impact or how it solves the problem for people who aren't familiar with the problem space. In a way that doesn't make it too crowded with words."

**Translation:** Add context and impact, but preserve the concise format.

### Reviewer 2 (Rating: 3/5)
> "The problem/solution descriptions are quite high level, and the visuals abstracted—so I didn't get a good sense of the detail of work in a real context... I didn't get a sense of the problem space/trade-offs/process etc. to get a sense of how you think and work with real world constraints."

**Translation:** Show decision-making and constraints, not just outcomes.

### Reviewer 3 (Rating: 4/5)
> "The craft is super strong, but I want something beyond just excellent production. Right now it reads a little like a very highly skilled senior... the most effective showcases I've seen usually have just slightly more context and impact narrative, not just problem/solution."

> "I want to know more about you and your unique approach to design... Right now it's feeling like your identity is 'I like making AI stuff', which is fine, but as a hiring manager I want to know why and what motivates you around that."

**Translation:** Show outcomes and strategic thinking. Answer "why you?"

---

## Hiring Manager Research: What "Lead-Level" Means in a Portfolio

Research across hiring manager interviews, design leadership articles, and portfolio reviews reveals consistent patterns for what separates senior from lead portfolios.

### The Senior vs Lead Distinction

| Senior Portfolio Shows | Lead Portfolio Shows |
|------------------------|----------------------|
| Expert craft | Vision and strategic direction |
| Deep product knowledge | Cross-team influence |
| Complex problem-solving | Organizational coordination |
| Individual ownership | Team enablement |
| Immediate priorities (0-6 months) | Long-term thinking (12+ months) |
| "I designed this feature" | "This changed how the team/org works" |

### What Hiring Managers Actually Look For

From research across multiple sources:

1. **Leadership signals over pure craft** — "Hiring managers expect strong design skills—it's leadership skills that really differentiate a senior portfolio from a junior one."

2. **Strategic decision-making** — "The working out is 90% of it—what decisions did you make and why? I want to see all those decisions articulated."

3. **Honest attribution** — "It may have been a group effort and that is fine—be honest about your contribution."

4. **Impact framing** — "Show the results first. Hiring managers are busy. If you show results first, they'll be more inclined to find out how you got there."

5. **The About page matters** — "The About page is the first place they click once they've decided your work is solid. They're no longer evaluating your process; they're evaluating you."

### The 6-8 Second Reality

Hiring managers spend 6-8 seconds on initial portfolio scans. The vignette format is well-suited for this—but the opening framing must signal business impact, not just project description.

### How to Signal Lead Level

1. **Show T-shaped leadership** — Breadth across responsibilities AND depth for outsized impact
2. **Include leadership artifacts** — Process improvements, strategic planning, knowledge-sharing
3. **Demonstrate influence** — Stakeholder management, building credibility, changing minds
4. **Show team-level outcomes** — Did you influence other projects? Change the design system? Shift company strategy?
5. **Company-first framing** — Multi-scale impact: business + users + team + culture

---

## The Three Missing Layers

Based on feedback and research, the vignettes need three additional content layers:

### Layer 1: Context (the "before")

**Answers:** Who, where, why now, what constraints?

**Current state:**
> "Synthesizing feedback was the most time-intensive part of performance reviews"

**What's missing:**
- Who experiences this? (Managers? HR? Both?)
- How time-intensive? (Hours? Days per review?)
- Why was this a priority now?
- What constraints shaped the timeline?

**Good context is specific and brief:**
> "Enterprise customers. Managers spending 2+ hours per direct report. Research showed they were already using ChatGPT as a workaround."

### Layer 2: Decisions & Trade-offs (the "how")

**Answers:** What did you consider? What did you decide against? Why?

Design notes currently focus on UI decisions:
> "Tuned the prompts to include specifics like project names, based on user feedback"

**What's missing:**
- What alternatives were on the table?
- What research led to this direction?
- What did you advocate for that wasn't obvious?

**Good decision framing shows thinking:**
> "PM wanted to cut verification features to hit the deadline. Research showed trust was the blocker, not speed. Advocated to keep it—became the pattern for all AI features."

### Layer 3: Impact (the "after")

**Answers:** What changed? For whom? By how much?

**Impact options, ranked by strength:**

| Type | Example | Strength |
|------|---------|----------|
| Hard metrics | "60% reduction in synthesis time" | Strongest |
| Adoption numbers | "12 designers, 30 prototypes created" | Strong |
| Behavioral shift | "Managers went from avoiding AI to requesting it" | Good |
| Strategic outcome | "Findings changed the H2 roadmap" | Good |
| Process change | "Became the standard pattern for AI features" | Acceptable |

**When you don't have metrics:**
> "Success criteria doesn't always have to be metrics—it can be landing a client, launching to get more investors onboard, etc. Showing the thinking alone gets you 80% of the way there."

---

## What the Research Validates

The vignette approach is the right direction. Research confirms:

- **60-80% visuals, 20-40% text** is the recommended ratio
- **Interactive, explorable experiences** are praised over static case studies
- **Compact format** respects the 6-8 second attention span
- **Show don't tell** is explicitly validated for senior portfolios

The format isn't the problem. The content framing is.

---

## Visual Hierarchy Feedback

Reviewer 3 also noted:
> "There's something a bit unresolved for me in the visual hierarchy. Once I start clicking around I kind of just want to keep clicking rather than taking each project in. I think having several primary actions visible at once contributes to that."

This is a separate issue from content depth, but worth noting for a future pass.

---

## Worksheet: Vignette Content Audit

Fill this out for each vignette to identify what's missing.

### AI Highlights

**Current problem statement:**
> Synthesizing feedback was the most time-intensive part of performance reviews

**Current solution statement:**
> Designed AI summaries managers could verify and trust

**Context:**
- **Who:** Managers doing performance reviews. Feedback synthesis came up as one of the most time-consuming and mentally draining parts of the process.
- **Why now:** Competitive pressure. Others were shipping AI features, we needed to move. This problem came out of earlier discovery as a good fit.
- **Constraint:** From the start, we wanted to keep human judgment central. Managers needed to be able to verify what the AI reported, not just trust it blindly.
- **Team/timeline:** ~2 months. Sole designer, working with PM, data scientist, FE/BE engineers. Lots of exec oversight since it was a hot project.

**Decisions:**
- **What I owned:** Took over after initial discovery. Owned the design through validation, iteration, and launch — defining how the feature looked, felt, and behaved.
- **Design direction:** AI features fail when they feel like black boxes. I focused on transparency (showing sources), specificity (concrete examples over generic summaries), and narrative structure ("Highlights and Opportunities" framing).
  - Defined how themes surfaced and the information hierarchy
  - Set the interaction model for AI-generated content
  - Worked with data scientist to tune prompts so insights were specific (project names, concrete examples) — not generic
- **Friction:** Early versions didn't land with execs. Lots of feedback, felt like the framing wasn't right. I had to refine the storytelling until it clicked. My improvements to the framing and how I presented the work eventually got buy-in.
- **Stakeholder work:** High-visibility project. Did exec presentations to build alignment and get approval.

**Impact:**
- The Coach team (our conversational AI assistant) adopted the "Highlights and Opportunities" structure for their product.
- Customer feedback validated the approach: "just the right amount of AI"
- 100% positive thumbs feedback

---

### AI Suggestions

**Current problem statement:**
> (Fill in from content.ts)

**Current solution statement:**
> (Fill in from content.ts)

**Context:**
- **Who:** Managers writing feedback. We knew feedback quality varied widely, and how feedback is written really matters.
- **Why now:** Competitive pressure. Others were shipping AI features, we needed to move. This was the first AI feature I worked on at Culture Amp.
- **Constraint:** Respect the manager's agency. Encourage critical thinking, not just "let the AI do it."
- **Team/timeline:** ~2 months. Sole designer, working with PM, data scientist, FE/BE engineers.

**Decisions:**
- **Design direction:** We were focused on maintaining agency from the start. The question wasn't whether to be proactive or reactive. It was how to make AI assistance feel like a tool you reach for, not something imposed on you.
- **Trade-off:** This was early 2024. The models simply weren't that good yet. That's why the approach was more minimal. We didn't trust the LLMs enough to be more aggressive.
- **What I owned:** Inline activation. The Improve button lives directly in the text editor, not buried in a menu or behind a separate mode.
- **Visual language:** I designed the AI gradient pattern as a way to signal "AI is helping here" without being intrusive. This became Culture Amp's standard signifier for AI features.

**Impact:**
- 80% of managers who used Improve made changes to their feedback.
- The AI gradient pattern I created here spread to other AI features across the product, including AI Highlights.

---

### Prototyping (Design Sandbox)

**Current problem statement:**
> Designers had no shared foundation for AI prototyping

**Current solution statement:**
> A shared sandbox that makes AI prototyping easy

**Existing impact data:**
> 12 designers, 30 prototypes

**Context:**
- **Who:** I was leading the way on AI prototyping at Culture Amp. As more designers started exploring this space, I could see we'd hit a wall. No way to deploy or share prototypes internally.
- **Why now:** No one asked me to build this. I saw the gap coming and filled it before it became a blocker.
- **Constraint:** Evaluated Replit but there was no budget. We had access to Claude Code, so I built around that.
- **Team/timeline:** Mostly solo, with some support from the DevEx team for infrastructure.

**Decisions:**
- **What I did:** No roadmap slot, no formal project. I just built it.
- **Adoption work:** Built the tool, then did the harder work of driving adoption. Documentation, onboarding sessions, walking teams through how to use it. That's what made it stick.

**Impact:**
- 12 designers across the org now actively use it. 30+ prototypes created.
- Before this, AI-assisted prototyping wasn't happening. We didn't have the infrastructure.
- One team used it to prototype a significant feature. The prototypes were how the team and executives got bought in on a significant company release.
- This shifted how designers at Culture Amp think about prototyping. From static mocks to functional code.

---

### Multilingual (Translation Management)

**Current problem statement:**
> Supporting multiple languages required a separate workflow for each language

**Current solution statement:**
> Designed a simple way to bring multiple languages into a single cycle

**Context:**
- **Who:** Enterprise customers running performance reviews across multiple countries. They were managing separate cycles per language. One customer was running twelve parallel cycles for what should have been one review period.
- **Why now:** Major blocker for our largest customers. Tied to significant ARR.
- **Constraint:** Integration, not invention. Admins already had a mental model for how performance cycles worked. The translation layer had to slot into that flow without requiring them to learn a new system.
- **Team/timeline:** A couple months of discovery, testing, and iterating. PM, Tech Lead, me, and devs.

**Decisions:**
- **What I advocated for:** Original scope was a basic translation upload flow. I pushed to include machine translation and Excel export based on what we saw in research. These weren't nice-to-haves. They were the actual workflow.
- **Discovery gap:** Before research, everyone assumed it was just translation management. No one knew machine translation and Excel were core to the workflow. Research revealed the actual process.
- **Research:** Customer interviews showed the same pattern across accounts. HR teams were already using machine translation as a first pass, then cleaning it up in Excel before uploading. We weren't introducing new tools. We were removing the manual steps around tools they already trusted.

**Impact:**
- Zero support tickets since launch.
- After launch, customers started asking for the same translation workflow in other parts of the product. The pattern we built became the reference for how Culture Amp should handle multilingual content.

---

### Home Connect

**Current problem statement:**
> (Fill in from content.ts)

**Current solution statement:**
> (Fill in from content.ts)

**Context:**
- **Who:** Managers using Culture Amp. They were missing signals about their direct reports. Goals going stale. Feedback sitting unread. The information existed in the product. It just wasn't surfaced where managers could act on it.
- **Why now:** Adoption numbers didn't match the value we knew was in the product. The homepage was the highest-traffic page. If managers weren't finding what they needed there, they weren't finding it.
- **Constraint:** We were limited to the data we could pull. The homepage could only surface a subset of Culture Amp, not everything. Had to design around what was available.
- **Team/timeline:** Standard team. PM, me, engineers.

**Decisions:**
- **What I advocated for:** The original approach was engineering-led. A dashboard where each product team gets their section. Goals here, feedback there, 1-on-1s somewhere else. It made sense from an org chart perspective. But it didn't match how managers think. They don't think in features. They think in people.
- **How I convinced stakeholders:** I had to deviate from the engineering-led approach. To get buy-in, we ran a small experiment first. That's where the 1-on-1 stats came from. The data convinced people.
- **The approach:** Instead of feature cards, the homepage shows your people. Direct reports with goals that need attention. Team members who received feedback. The actions you need to take show up in context of who they're about.

**Impact:**
- 255% increase in the weekly number of managers scheduling a 1-on-1 for the first time. We weren't expecting something of that magnitude. The experiment validated the approach was working.
- Customer quote: "I think as a leader, you want things here that are gonna prompt you to ask questions. And this does that. Seeing people that have received feedback and seeing that someone has an inactive goal gives me the prompt to ask a question. This is great."

---

### Vibe Coding

**Current problem statement:**
> (Fill in from content.ts)

**Current solution statement:**
> (Fill in from content.ts)

**Context:**
- **What motivated this:** Learning new tech. Figuring out how to build things on my own. I love exploring new tools, and I've got an eye on doing my own thing at some point.
- **What I built:** A prototyping tool. You give it an idea and it builds out a prototype for you. It has versioning for different ideas, and a prototype "scrubber" that lets you play prototypes like a movie. Like a v0/Lovable clone, but one I built for myself.
- **Technical challenges:** Figured out how to bring a coding agent CLI into a browser context. Designed a whole set of interactions around bringing that behavior to life in a web app. Also designed the "Scrubber" because a real prototype isn't as observable as a bunch of Figma sheets. You can't easily review or compare states. The scrubber lets you play through a prototype like a movie, making the work reviewable again.

**What I learned:**
- You can do so much in a short amount of time. It's time to raise the ambition of what we can do.
- Actual time building with the tools is the most critical thing in the AI world. You have to tinker, learn, and experiment.
- This is how I learned to build the Design Sandbox at work. The personal project was the proving ground.

**Takeaway:**
- I can design and build. I learn new things on my own. I have deep fluency in these tools that comes from actual experience, not just curiosity.

---

## Identity Questions (For About Section)

Reviewer 3: "I want to know more about you and your unique approach to design."

**Why AI? What draws you to this space?**
AI fundamentally reworks software. We are still so early in figuring out how to really help people use these tools. It also reworks how we build software. People like me can effectively research, wireframe, prototype and then also have a hand in building the software.

**What do you believe that most designers don't?**
With these tools I'm designing in the material itself: code, rather than an approximation via traditional design tools. By material I mean code running in a browser that I can click, break, and fix. Direct contact with the material lets me feel what's working, what isn't, and where to push. I notice pacing and rhythm, pick up awkward moments earlier, and judge how it feels to use.

**What kind of problems energize you?**
Pairing software with real people problems. I strive to design things that just feel right. And I know how much work that requires. It's not about interaction challenges in the abstract. It's about making complex systems feel obvious to the people using them. Strategy matters, but so does execution. Both.

---

## Next Steps

1. **Fill out the worksheet** for each vignette
2. **Prioritize 2-3 vignettes** that have the strongest answers
3. **Draft new content** for those vignettes first
4. **Test with 1-2 people** before rolling out to all vignettes
5. **Address identity questions** for About section (separate effort)

---

## Sources

- Feedback CSV from portfolio survey (3 respondents, Jan 7-9 2026)
- [The Crucial First Impression - Medium](https://medium.com/design-bootcamp/the-crucial-first-impression-what-hiring-managers-look-for-in-product-design-portfolios-553e6a865ee9)
- [What a Hiring Manager Looks For - Leslie Yang](https://leslieyang.substack.com/p/what-a-hiring-manager-looks-for-in)
- [How Recruiters Actually Look at Your Portfolio](https://blog.opendoorscareers.com/p/how-recruiters-and-hiring-managers-actually-look-at-your-portfolio)
- [Impact-First Storytelling in Your Product Design Portfolio](https://medium.com/@sarahscussel/impact-first-storytelling-in-your-product-design-portfolio-9f122f747ee8)
- [What's the Difference Between Lead, Staff, and Principal Designer?](https://newsletter.uxdesign.cc/p/whats-the-difference-between-lead)
- [Three Tips on Design Leaders' Portfolio Presentations - DoorDash](https://medium.com/design-doordash/three-tips-on-design-leaders-portfolio-presentations-5afd4e412bf8)
- [4 Product Design Portfolio Mistakes - Medium](https://medium.com/design-bootcamp/4-product-design-ux-portfolio-mistakes-that-silently-ruin-your-job-search-3e6a666a48e7)
