# Performance AI: Evolving Design Principles for AI-Powered Manager Tools
*From foundational AI explorations to sophisticated, user-validated solutions that established Culture Amp's AI design language*

## Overview
As Culture Amp entered the AI era, we needed to demonstrate our AI capabilities while solving real manager pain points in performance reviews. I led the design of Performance AI - two interconnected features that evolved our understanding of AI UX through innovative research methodology and technical collaboration. Using synthetic data prototyping and Python-based prompt engineering, I developed systematic approaches to AI feature validation that established design principles now used across the company. The gradient border treatment I designed to signal AI features has become Culture Amp's standard AI visual language.

**My Role:** Lead Product Designer  
**Timeline:** [Duration] - [Time period]  
**Team:** 2 supporting designers, Product Manager, Engineering, Data Science, People Science team

![Performance AI overview - Replace with hero visual showing both features]

## The Challenge

Culture Amp needed to prove our "People Science" leadership in the emerging AI landscape. We identified two critical problems in manager performance reviews where AI could make meaningful impact:

1. **Review Quality Problem:** Managers struggle to write high-quality feedback that follows People Science best practices (specific, impactful, actionable, objective)
2. **Data Collation Problem:** Managers waste significant time gathering information from multiple sources (Google Docs, Slack, goals, 1-on-1 notes, external KPIs) to build a complete picture of their direct reports

The challenge was designing AI solutions that enhanced human judgment rather than replacing it, while navigating competing stakeholder expectations from ambitious leadership and conservative data science practices. This required developing new research methodologies for testing AI features and building technical capabilities to iterate on prompts and AI outputs directly.

![Business context visualization - Replace with problem statement visual]

## Getting deeper into the problem

I leveraged existing user research from our research team about feedback quality, while conducting my own research on the manager review-writing process to understand the data collation challenge.

### What I learnt from managers about their review process

Through interviews and process observation, I discovered:

1. **Cognitive overhead is massive** - Managers described spending hours just remembering and gathering relevant information about each employee before they could even start writing
2. **Validation behavior, not creation** - "I use all this context to validate my own viewpoints about this person's performance"  
3. **Time sink frustration** - The collation process was consistently described as the most time-consuming part of review writing
4. **Quality awareness gap** - Managers knew their feedback could be better but struggled to identify specific improvements

### Organizational tensions to navigate

The project required balancing multiple competing perspectives:
- **Senior leadership:** Pushing for ambitious AI capabilities to demonstrate market leadership
- **Data science team:** Conservative approach due to concerns about AI overreach
- **People Science team:** Ensuring AI recommendations aligned with research-backed feedback principles
- **User needs:** Practical solutions that saved time while maintaining manager agency

## Design Process & Evolution

Based on my research insights, I approached this as two phases that would inform each other, establishing core AI design principles that could evolve:

### Phase 1: Foundational AI Exploration - Suggested Improvements

For the review quality challenge, I designed around the People Science framework of "Specific, Impactful, Suggest actions & Be objective" - building these qualities directly into the AI prompts.

**Key Design Decisions:**
1. **Preserve manager agency** - Instead of AI writing feedback, we provided improvement suggestions that managers had to implement themselves
2. **Transparent reasoning** - I advocated against data science's preference for simple pass/fail indicators, pushing for explanations of why quality standards weren't met
3. **AI visual language** - Created a gradient border treatment to clearly signal AI-powered features

**Design Principle:** *Trust through transparency and human control*

![Suggested Improvements workflow - Replace with actual screens]
> Early AI exploration prioritizing user agency and transparent recommendations

### Phase 2: Sophisticated AI Integration - Highlights & Opportunities  

Learning from Phase 1, I led a more user-centered approach for the data collation challenge, establishing advanced AI UX principles:

**Core Design Principles:**
1. **Transparency & Verifiability** - Users must be able to check the AI's work
2. **Structured comprehension** - Break complex summaries into digestible, themed sections
3. **Source attribution** - Always show where information came from

**Key Design Features:**
- **Collapsible theme structure** - Organized peer and upward feedback into clear categories
- **Source verification** - Click-through to original feedback for fact-checking
- **Synthetic data prototyping** - Created customized prototypes using participants' job roles and synthetic direct report data
- **Prompt engineering through design** - Built Jupyter notebooks to test prompt variations, blending design, content design, and data science

![Highlights & Opportunities interface - Replace with actual screens]
> Evolved AI approach emphasizing verification and structured information architecture

### Advanced AI UX methodology

For Phase 2, I developed a sophisticated research and iteration process that bridged design and data science:

**Synthetic data prototyping process:**
1. Interviewed managers about their roles and team structures
2. Created realistic synthetic direct report profiles matching their context
3. Ran synthetic data through our actual AI prompts to generate realistic outputs
4. Tested these customized prototypes with participants for authentic feedback

**Technical prompt optimization:**
Using my Python skills and LLM coding tools, I built Jupyter notebooks to systematically test prompt variations against our synthetic datasets. This allowed me to iterate on prompts with the rigor of data science while maintaining focus on user experience outcomes.

**Cross-functional advocacy:**
- **Strategic compromise** - Finding solutions that satisfied ambitious leadership while respecting data science constraints  
- **Evidence-based advocacy** - Using both user research and technical testing to push back on feedback that didn't serve user needs
- **Technical credibility** - My ability to work directly with prompts and data gave me stronger influence in technical discussions

## Results & Impact

### Suggested Improvements (Phase 1)
- **80% effectiveness rate** - 80% of feature uses correlated with edits made to reviews, demonstrating clear user value
- **Foundational learning** - Established that conservative AI approaches could still deliver meaningful user value
- **Design system impact** - Gradient border AI treatment adopted as company-wide standard for AI features

### Highlights & Opportunities (Phase 2) 
- **User validation success** - When provided easy verification tools, 100% of users checked AI work, validating our transparency approach
- **Prompt evolution** - User testing revealed need for more detailed themes including projects and metrics
- **Scalable architecture** - Designed system to accommodate future data sources and AI capabilities

### Broader organizational impact
- **AI design leadership** - My gradient border treatment became Culture Amp's standard AI visual language
- **Cross-team alignment** - Successfully bridged tensions between ambitious leadership and conservative engineering
- **Design principle establishment** - Created reusable framework for future AI feature development

## Reflection & Evolution

This project taught me that successful AI UX requires balancing multiple tensions:

**Early AI adoption lessons:** Phase 1 showed that even conservative AI approaches can deliver user value when designed thoughtfully. The 80% edit rate proved that improvement suggestions, while not revolutionary, provided real utility.

**AI UX maturity:** Phase 2 demonstrated how sophisticated research methodology can dramatically improve AI implementations. By combining synthetic data prototyping with technical prompt engineering, I was able to iterate with both user empathy and technical precision. The shift from simple summarization to structured, verifiable themes shows the importance of understanding user mental models while having the technical skills to implement solutions.

**Organizational navigation:** Perhaps most importantly, I learned how to advocate for user needs while managing competing stakeholder expectations - a critical skill for AI product development where the stakes and uncertainty are high.

**Design system thinking:** The gradient border becoming company-wide AI language proved that thoughtful micro-interactions in AI features can scale to influence entire product ecosystems.

![Final implementation comparison - Replace with before/after or progression visual]

---

*This case study demonstrates the evolution from cautious AI exploration to sophisticated, user-validated AI UX - establishing principles and visual language that continue to guide Culture Amp's AI product development.*